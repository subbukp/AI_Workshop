# AI Workshop - Day 01
**Date:** 10 Dec 2025

## Trainer
- **Rakesh Bhugra**: [Website](https://www.rakeshbhugra.com/)

## Key Concepts

### LLM Evolution Timeline
**In-depth coverage**: [LLM Evolution Timeline](https://www.rakeshbhugra.com/blog/llm-evolution-timeline)
_Detailed explanation provided during the session_

### Evaluation Models
An evaluation framework is a structured approach used to assess the performance, quality, or impact of a system, program, or model across various dimensions. In the context of large language models (LLMs), an evaluation framework provides a standardized method for testing and measuring the outputs of LLMs or systems built using LLMs. These frameworks are essential for ensuring reliability, consistency, and safety in AI applications, particularly as models evolve through continuous updates.

## Tools & Resources

### UI & Design Tools
- **Comfy UI**: [Cloud Platform](https://www.comfy.org/cloud)
- **Polymet.ai**: AI product designer, at your service

### Documentation & Frameworks
- **LangGraph Multi-Agent Workflows**: [Blog Post](https://blog.langchain.com/langgraph-multi-agent-workflows/)
- **OpenAI Documentation**: [Overview](https://platform.openai.com/docs/overview)
- **LangChain LangGraph**: [Official Site](https://www.langchain.com/langgraph)
- **Pydantic AI - OpenAI Models**: [Documentation](https://ai.pydantic.dev/models/openai/)

## Notes
_Workshop notes and learnings to be added throughout the day..._
